# -*- coding: utf-8 -*-
"""ANN Churn Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f_OTICYpZ-4tTq4m-sFE8Bt3yTdNVb8v
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pickle

data = pd.read_csv('/content/Churn_Modelling.csv')
data.head()

# Dropping irrevelant features
data = data.drop({'RowNumber', 'CustomerId', 'Surname'}, axis = 1)

# Encoding categorical variable using label encoder because 2 features only
le = LabelEncoder()
data['Gender'] = le.fit_transform(data['Gender'])

# Since more than 2 feature in Geography so using one hot encoder
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
geo_encoder = ohe.fit_transform(data[['Geography']])
geo_encoder

geo_encoder.toarray()

geo_df = pd.DataFrame(geo_encoder.toarray(), columns = ohe.get_feature_names_out(['Geography']))
geo_df.head()

# Now concatinating the OHE dataframe with the main dataframe
data = pd.concat([data.drop('Geography', axis = 1), geo_df], axis = 1)

data.head()

# Save the encoders
with open('le.pkl', 'wb') as file:                  # wb used for writing
  pickle.dump(le, file)

with open('ohe.pkl', 'wb') as file:
  pickle.dump(ohe, file)

# Splitting the data into dependent and independent variable and applying train test split
x = data.drop({'Exited'}, axis = 1)
y = data['Exited']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

# Scaling the features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Saving scaler
with open('scaler.pkl', 'wb') as file:
  pickle.dump(scaler, file)

"""#***ANN Implementation***"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
import datetime

model = Sequential([
    Dense(64, activation = 'relu', input_shape = (x_train.shape[1], )),      # Hidden layer 1
    Dense(32, activation = 'relu'),                                          # Hidden layer 2
    Dense(1, activation = 'sigmoid')                                         # Output layer
]
)

model.summary()

# Compiling the model
model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Setup Tensorboard
log_dir = 'log/fit/' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorflow_callback = TensorBoard(log_dir = log_dir, histogram_freq = 1)

# Early stopping used for stopping the epochs if the accuracy is not incresing after certain interval
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)

# train the model
history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 100, callbacks = [tensorflow_callback, early_stopping_callback])

model.save('model.keras')
model.save('model.h5')

# Commented out IPython magic to ensure Python compatibility.
# Load tensorboard extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir log/train20241229-063330

"""#***Prediction***"""

# Example Input data
input_data = {
    'CreditScore' : [600],
    'Gender' : ['Male'],
    'Age' : [40],
    'Tenure' : [3],
    'Balance' : [6000],
    'NumOfProducts' : [2],
    'HasCrCard' : [1],
    'IsActiveMember' : [1],
    'EstimatedSalary' : [50000],
    'Geography' : ['France']
}

# preprocessing the data
input_data = pd.DataFrame(input_data)
input_data['Gender'] = le.transform(input_data[['Gender']])
geo_input_data = ohe.transform(input_data[['Geography']]).toarray()
geo_input_data = pd.DataFrame(geo_input_data, columns = ohe.get_feature_names_out(['Geography']))

# concatinating both the dataframes
input_data = pd.concat([input_data.drop({'Geography'}, axis = 1), geo_input_data], axis = 1)

# Scaling the user input data
input_scaled = scaler.transform(input_data)

"""#***Prediction***"""

# Prediction for churn
prediction = model.predict(input_scaled)

prediction

if prediction > 0.5:
  print(f"The customer is likely to churn")

else:
  print(f"The customer is not likely to churn")

pip install streamlit

# Stremlit App
import streamlit as st

st.title("Customer Churn Prediction")

# User inputs
geography = st.selectbox('Geography', ohe.categories_[0])
gender = st.selectbox('Gender', le.classes_)
age = st.slider('Age', 18, 92)
balance = st.number_input('Balance')
credit_score = st.number_input('CreditScore')
estimated_salary = st.number_input('EstimatedSalary')
teniure = st.slider('Tenure', 0, 10)
no_of_products = st.slider('NumOfProducts', 1, 4)
has_cr_card = st.selectbox('HasCrCard', [0, 1])
is_active_member = st.selectbox('IsActiveMember', [0, 1])

# Prepare input data
input_data = pd.DataFrame({
        'Geography': [geography],
        'CreditScore' : [credit_score],
        'Gender' : [le.transform([gender])[0]],
        'Age' : [age],
        'Tenure' : [teniure],
        'Balance' : [6000],
        'NumOfProducts' : [no_of_products],
        'HasCrCard' : [has_cr_card],
        'IsActiveMember' : [is_active_member],
        'EstimatedSalary' : [50000],
})

geo_input_data = ohe.transform(input_data[['Geography']]).toarray()
geo_input_data = pd.DataFrame(geo_input_data, columns = ohe.get_feature_names_out(['Geography']))

# concatinating both the dataframes
input_data = pd.concat([input_data.reset_index(drop=True), geo_input_data], axis=1)
input_data.drop(columns=['Geography'], inplace=True)

# Scaling the user input data
input_scaled = scaler.transform(input_data)

# Prediction for churn
prediction = model.predict(input_scaled)

if prediction > 0.5:
  print(f"The customer is likely to churn")

else:
  print(f"The customer is not likely to churn")